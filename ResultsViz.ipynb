{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results_summary.dict', 'rb') as f:\n",
    "    stats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latex(df):\n",
    "    print(df.to_latex().replace('\\$', '$'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_table(results_dict, metrics, keep_methods):\n",
    "    d = {}\n",
    "    missing_methods = []\n",
    "    for method in keep_methods:\n",
    "        if method in results_dict:\n",
    "            d[method] = ['{:.4f} ({:.4f})'.format(*results_dict[method][metric]) if 'time' in metric else \\\n",
    "                         '{:.4f} ({:.4f})'.format(*results_dict[method][metric]) for metric in metrics]\n",
    "        else:\n",
    "            missing_methods.append(method)\n",
    "    df = pd.DataFrame.from_dict(d, orient='index')\n",
    "    df.columns = metrics\n",
    "    df.index.names = ['alg']\n",
    "    if len(missing_methods) > 0:\n",
    "        print('missing methods: {}'.format(missing_methods))\n",
    "    return df.loc[[x for x in keep_methods if x not in missing_methods]], missing_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_results_table(results_dict, exper_type, num=5):\n",
    "    metrics = ['test_eval', \n",
    "               'test_eq_max', 'test_eq_mean', \n",
    "               'test_ineq_max', 'test_ineq_mean',\n",
    "               'test_time']\n",
    "    metrics_raw = [x.replace('test_', 'test_raw_') for x in metrics]\n",
    "    metrics_renaming = ['Obj. value', \n",
    "                        'Max eq viol.', 'Mean eq viol.', \n",
    "                        'Max ineq viol.', 'Mean ineq viol.',\n",
    "                        'Time']\n",
    "    \n",
    "    # nn_methods = ['method', 'method_no_compl', 'method_no_corr', 'method_no_soft',\n",
    "    #            'baseline_nn', 'baseline_eq_nn']\n",
    "    nn_methods = []\n",
    "    opt_methods = dict([\n",
    "            ('simple', ['osqp', 'qpth']), ('nonconvex', ['ipopt']), ('acopf', ['pypower'])\n",
    "    ])\n",
    "    opt_methods_list = ['baseline_opt_{}'.format(x) for x in opt_methods[exper_type]]\n",
    "    methods_renaming_dict = dict((\n",
    "            ('method', 'DC3'),\n",
    "            ('method_no_compl', 'DC3, $\\neq$'),\n",
    "            ('method_no_corr', 'DC3, $\\not\\leq$ train'),\n",
    "            ('method_no_corr-noTestCorr', 'DC3, $\\not\\leq$ train/test'),\n",
    "            ('method_no_soft', 'DC3, no soft loss'),\n",
    "            ('baseline_nn-noTestCorr', 'NN'),\n",
    "            ('baseline_nn', 'NN, $\\leq$ test'),\n",
    "            ('baseline_eq_nn-noTestCorr', 'Eq.~NN'),\n",
    "            ('baseline_eq_nn', 'Eq.~NN, $\\leq$ test'),\n",
    "            ('baseline_opt_osqp', 'Optimizer (OSQP)'),\n",
    "            ('baseline_opt_qpth', 'Optimizer (qpth)'),\n",
    "            ('baseline_opt_ipopt', 'Optimizer (IPOPT)'),\n",
    "            ('baseline_opt_pypower', 'Optimizer (PYPOWER)')\n",
    "        ))\n",
    "    \n",
    "    df1, missing_methods = get_results_table(results_dict, metrics, nn_methods + opt_methods_list)\n",
    "    \n",
    "    # df2, missing_methods_2 = get_results_table(results_dict, metrics_raw, nn_methods)\n",
    "    # df2.columns = metrics\n",
    "    # df2.index =  ['{}-noTestCorr'.format(x) for x in df2.index]\n",
    "    \n",
    "    # missing_methods = missing_methods + ['{}-noTestCorr'.format(x) for x in missing_methods_2]\n",
    "        \n",
    "    # all_results = pd.concat([df1, df2])\n",
    "    all_results = df1\n",
    "    # results_ordering = opt_methods_list + ['method', 'method_no_compl', 'method_no_corr', 'method_no_corr-noTestCorr',\n",
    "    #                    'method_no_soft', 'baseline_nn-noTestCorr', 'baseline_nn', 'baseline_eq_nn-noTestCorr',\n",
    "    #                    'baseline_eq_nn']\n",
    "    results_ordering = opt_methods_list\n",
    "    all_results = all_results.loc[[x for x in results_ordering if x not in missing_methods]]\n",
    "    all_results.index = [methods_renaming_dict[x] for x in all_results.index]\n",
    "    all_results.columns = metrics_renaming\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_simple_5050 = get_full_results_table(stats['simple_ineq50_eq50'], exper_type='simple')\n",
    "# df_simple_5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nonconvex = get_full_results_table(stats['nonconvex'], exper_type='nonconvex')\n",
    "# df_nonconvex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                           Obj. value     Max eq viol.    Mean eq viol.  \\\nOptimizer (PYPOWER)  13.4608 (0.0000)  0.0662 (0.0000)  0.0137 (0.0000)   \n\n                      Max ineq viol.  Mean ineq viol.             Time  \nOptimizer (PYPOWER)  0.0019 (0.0000)  0.0001 (0.0000)  0.7115 (0.0000)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Obj. value</th>\n      <th>Max eq viol.</th>\n      <th>Mean eq viol.</th>\n      <th>Max ineq viol.</th>\n      <th>Mean ineq viol.</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Optimizer (PYPOWER)</th>\n      <td>13.4608 (0.0000)</td>\n      <td>0.0662 (0.0000)</td>\n      <td>0.0137 (0.0000)</td>\n      <td>0.0019 (0.0000)</td>\n      <td>0.0001 (0.0000)</td>\n      <td>0.7115 (0.0000)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acopf = get_full_results_table(stats['acopf'], exper_type='acopf')\n",
    "df_acopf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix tables (simple problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comparison_table(all_stats, experiments, metrics, keep_methods):\n",
    "    d = {}\n",
    "    for method in keep_methods:\n",
    "        for metric in metrics:\n",
    "            d[(method, metric)] = {}\n",
    "            for experiment in experiments:\n",
    "                d[(method, metric)][experiment] = '{:.2f} ({:.2f})'.format(*all_stats[experiment][method][metric])\n",
    "        df = pd.DataFrame.from_dict(d, orient='index')           \n",
    "    df.index.names = ['Alg', 'Metric']\n",
    "    return df.iloc[df.index.isin(keep_methods, 'Alg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_comparison_table(all_stats, vary_across):\n",
    "    assert vary_across in ['ineq', 'eq'], 'vary_across should be in [ineq, eq]'\n",
    "    \n",
    "    if vary_across == 'ineq':\n",
    "        experiments = ['simple_ineq10_eq50', 'simple_ineq30_eq50', 'simple_ineq50_eq50', 'simple_ineq70_eq50',\n",
    "                       'simple_ineq90_eq50']\n",
    "        experiments_renaming = ['10', '30', '50', '70', '90']\n",
    "    else:\n",
    "        experiments = ['simple_ineq50_eq10', 'simple_ineq50_eq30', 'simple_ineq50_eq50', 'simple_ineq50_eq70',\n",
    "                       'simple_ineq50_eq90']\n",
    "        experiments_renaming = ['10', '30', '50', '70', '90']\n",
    "    \n",
    "    methods = ['method', 'method_no_compl', 'method_no_corr', 'method_no_soft', \n",
    "               'baseline_nn', 'baseline_eq_nn', 'baseline_opt_osqp']\n",
    "    methods_renaming_dict = dict((\n",
    "            ('method', 'DC3'),\n",
    "            ('method_no_compl', 'DC3, $\\neq$'),\n",
    "            ('method_no_corr', 'DC3, $\\not\\leq$ train'),\n",
    "            ('method_no_corr-noTestCorr', 'DC3, $\\not\\leq$ train/test'),\n",
    "            ('method_no_soft', 'DC3, no soft loss'),\n",
    "            ('baseline_nn-noTestCorr', 'NN'),\n",
    "            ('baseline_nn', 'NN, $\\leq$ test'),\n",
    "            ('baseline_eq_nn-noTestCorr', 'Eq.~NN'),\n",
    "            ('baseline_eq_nn', 'Eq.~NN, $\\leq$ test'),\n",
    "            ('baseline_opt_osqp', 'Optimizers (OSQP, qpth)')\n",
    "        ))\n",
    "    \n",
    "    metrics = ['test_eval', 'test_eq_max', 'test_ineq_max']\n",
    "    metrics_renaming_dict = dict((('test_eval', 'Obj. val.'), ('test_eq_max', 'Max eq.'), ('test_ineq_max', 'Max ineq.')  ))\n",
    "    clean_metrics = [metrics_renaming_dict[x] for x in metrics]\n",
    "    metrics_raw = [x.replace('test_', 'test_raw_') if 'time' not in x else x for x in metrics]\n",
    "    \n",
    "    df1 = get_comparison_table(all_stats, experiments, metrics, methods)\n",
    "    df1 = df1.reset_index()\n",
    "    df1['Metric'] = df1['Metric'].str.replace('_obj_val', '_eval') # make naming consistent\n",
    "    df1 = df1.set_index(['Alg', 'Metric'])\n",
    "    \n",
    "    df2 = get_comparison_table(all_stats, experiments, metrics_raw, methods[:-1])\n",
    "    df2.columns = experiments\n",
    "    df2 = df2.reset_index()\n",
    "    df2['Alg'] = ['{}-noTestCorr'.format(x) for x in df2['Alg']]\n",
    "    df2['Metric'] = df2['Metric'].str.replace('raw_', '')  # make naming consistent\n",
    "    df2 = df2.set_index(['Alg', 'Metric'])\n",
    "    \n",
    "    all_results = pd.concat([df1, df2])\n",
    "    results_ordering = ['baseline_opt_osqp', 'method', 'method_no_compl', 'method_no_corr', 'method_no_corr-noTestCorr',\n",
    "                       'method_no_soft', 'baseline_nn-noTestCorr', 'baseline_nn', 'baseline_eq_nn-noTestCorr', \n",
    "                       'baseline_eq_nn']\n",
    "    clean_results_ordering = [methods_renaming_dict[x] for x in results_ordering]\n",
    "    all_results = all_results.iloc[all_results.index.isin(results_ordering, 'Alg')]\n",
    "    all_results = all_results.reindex(pd.MultiIndex.from_product([results_ordering, metrics], names=['Alg', 'Metric']))\n",
    "    all_results = all_results.reset_index()\n",
    "    all_results['Alg'] = [methods_renaming_dict[x] for x in all_results['Alg']]\n",
    "    all_results['Metric'] = [metrics_renaming_dict[x] for x in all_results['Metric']]\n",
    "    all_results = all_results.set_index(['Alg', 'Metric'])\n",
    "    all_results.columns = experiments_renaming\n",
    "    all_results = all_results.reindex(pd.MultiIndex.from_product([clean_results_ordering, clean_metrics])) # remove names\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'simple_ineq50_eq10'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[106], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_simple_varyeq \u001B[38;5;241m=\u001B[39m \u001B[43mget_simple_comparison_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstats\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meq\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m df_simple_varyeq\n",
      "Cell \u001B[0;32mIn[105], line 33\u001B[0m, in \u001B[0;36mget_simple_comparison_table\u001B[0;34m(all_stats, vary_across)\u001B[0m\n\u001B[1;32m     30\u001B[0m clean_metrics \u001B[38;5;241m=\u001B[39m [metrics_renaming_dict[x] \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m metrics]\n\u001B[1;32m     31\u001B[0m metrics_raw \u001B[38;5;241m=\u001B[39m [x\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest_\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest_raw_\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m x \u001B[38;5;28;01melse\u001B[39;00m x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m metrics]\n\u001B[0;32m---> 33\u001B[0m df1 \u001B[38;5;241m=\u001B[39m \u001B[43mget_comparison_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mall_stats\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexperiments\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethods\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m df1 \u001B[38;5;241m=\u001B[39m df1\u001B[38;5;241m.\u001B[39mreset_index()\n\u001B[1;32m     35\u001B[0m df1[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMetric\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df1[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMetric\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_obj_val\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_eval\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;66;03m# make naming consistent\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[104], line 7\u001B[0m, in \u001B[0;36mget_comparison_table\u001B[0;34m(all_stats, experiments, metrics, keep_methods)\u001B[0m\n\u001B[1;32m      5\u001B[0m         d[(method, metric)] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m      6\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m experiment \u001B[38;5;129;01min\u001B[39;00m experiments:\n\u001B[0;32m----> 7\u001B[0m             d[(method, metric)][experiment] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{:.2f}\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{:.2f}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;241m*\u001B[39m\u001B[43mall_stats\u001B[49m\u001B[43m[\u001B[49m\u001B[43mexperiment\u001B[49m\u001B[43m]\u001B[49m[method][metric])\n\u001B[1;32m      8\u001B[0m     df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame\u001B[38;5;241m.\u001B[39mfrom_dict(d, orient\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m'\u001B[39m)           \n\u001B[1;32m      9\u001B[0m df\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39mnames \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAlg\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMetric\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[0;31mKeyError\u001B[0m: 'simple_ineq50_eq10'"
     ]
    }
   ],
   "source": [
    "df_simple_varyeq = get_simple_comparison_table(stats, 'eq')\n",
    "df_simple_varyeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simple_varyineq = get_simple_comparison_table(stats, 'ineq')\n",
    "df_simple_varyineq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LaTeX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_latex(df_simple_5050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_latex(df_nonconvex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_latex(df_acopf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_latex(df_simple_varyeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_latex(df_simple_varyineq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
